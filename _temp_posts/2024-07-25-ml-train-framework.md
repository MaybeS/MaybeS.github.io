---
title: Machine Learning Train Framework
date: 2024-07-25 12:00:00
description: Machine Learning Train Framework 구축을 통한 실험 결과 재현 가능성 및 생산성 향상
tag: machine lerning, train, framework, kedro, pytorch lightning, pre-commit, test
author: jiunbae
category: post
---

#### NFace 학습 프레임워크

제가 속했던 **NFace** 팀은 데이터 관리 및 생산성 향상을 위해 복잡한 데이터 및 머신러닝 파이프라인 구축을 용이하게 하고 모듈식으로 관리할 수 있는 오픈소스 프레임워크인 [Kedro](https://kedro.org/)를 사용하고 있었습니다.
_Kedro는 데이터 과학자들이 프로젝트를 구조화하고 관리하는 데 도움을 주는 파이썬 프레임워크로, 데이터 과학 프로젝트의 복잡성을 줄이고 생산성을 향상시키기 위해 설계되었습니다._ 그러나 Kedro를 활용하면서 익숙하지 않은 패턴으로 개발을 진행하게 되면서, 여러 빠르게 변경되는 데이터 소스와 파이프라인을 일관되게 모듈화하기 어려운 문제들이 발생했습니다. 그 결과, 간단하고 빠른 실험을 위해 연구원 개인이 별도의 스크립트를 따로 만들어서 실험하는 경우가 많았습니다. 이렇게 코드가 일원화되지 않자, 실험 결과의 재현이 어렵고 모듈을 합치기 곤란한 상황들이 자주 발생하였습니다.

저는 조직 개편으로 잠깐 시간이 생긴 틈을 타서 이러한 문제를 해결하고, 실험 결과를 재현할 수 있으면서도 쉽고 빠르게 실험할 수 있는 환경을 만들고자 했습니다. 처음에는 Kedro를 사용하던 기존의 코드를 가볍고 빠른 실험을 위한 간단한 도구로 리팩토링하고, 유용한 기능들을 추가하는 것으로 충분할 것이라 생각했으나, 고려해야 할 부분이 많아지면서 기존 계획보다 더 많은 부분을 고려해야 했습니다. 가장 중점적으로 고려한 항목은 다음과 같습니다.

1. 실험 결과를 재현 가능할 것

   - 저는 언제나 이 항목이 모든 연구에서 가장 중요하다고 생각합니다.

2. 누구에게나 동일한 학습 환경을 제공할 것

   - 특히 파이썬 패키지 매니저의 미흡한 지원으로 버전 충돌과 같은 오류가 자주 발생합니다.
   - 빠르게 실험을 진행하다보면 로직이 산재해 있고 파라메터도 잘 관리되지 않기 때문에 이를 일원화 할 필요가 있었습니다.

3. 빠르게 모델을 실험하고 프로토타이핑 할 수 있을 것
   - 기존 환경은 동작하기 위한 최소한의 처리가 너무 무거웠기 때문에 기피하는 경향이 있었습니다.

그 밖에도 실제로 연구 개발하고 서비스를 배포하면서 필요하다고 느낀 **_서비스에 배포된 모델과 실험 결과가 동일할 것_**, **_실험 결과가 동일한지 쉽게 확인할 수 있을 것_**, **_내가 실험한 결과를 다른 사람도 간편하게 테스트 할 수 있을 것_**, **_로깅 도구들로 실험 결과를 쉽게 파악할 수 있을 것_** 등이 있었습니다.

문제점들은 파악했지만, 어떻게 해결해야 하는지와 어떤 방법들이 효과적인지는 알기 어려웠지만 아래와 같은 해결책을 적용해 나가기 시작했습니다.

먼저 실험 결과를 재현하기 위해 사용하는 변수를 모두 저장하고, 각 로직이 일관된 변수를 사용할 수 있도록 만들었습니다. 이를 쉽게 구현하기 위해 연구 환경에서의 모든 설정을 전역 Config 싱글톤 클래스로 관리하고, 각 함수에 Config가 함수 파라미터 이름을 통해 설정 값을 찾아 자동으로 이를 채워 넣는 방식으로 작동하도록 했습니다. 따라서 각 로직이 같은 이름의 파라미터를 사용한다면 같은 값이 들어올 것을 보장하여, 각 파라미터 이름을 일원화하고 어떤 파라미터가 최종적으로 사용되었는지 확인할 수 있도록 했습니다. 이를 통해 일관된 파라미터 이름을 사용하고 파라미터가 어떤 값으로 사용되었는지 쉽게 확인할 수 있도록 만들었습니다.

또한 동일한 환경에서 실험할 수 있도록 데이터셋을 일원화하고 이름을 붙여서 쉽고 고정된 형태로 관리될 수 있도록 만들었습니다. 이를 S3 스토리지와 NFS로 제공해 연구에 사용되는 디바이스에서 접근 가능하도록 만들어 필요한 데이터를 자동으로 받아오고, 완료된 실험들을 자동으로 업로드하는 방식으로 구현했습니다. 이를 통해 새로운 디바이스에서도 쉽게 설치하고 동일한 환경에서 빠르게 실험과 학습, 추론을 진행할 수 있도록 만들었습니다.

또한 [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)을 도입하여 연구자들이 관리해야 할 부분을 최소화했습니다. PyTorch Lightning은 PyTorch의 High-level API들을 제공하여 실제로 연구해야 하는 부분에 집중할 수 있도록 여러 처리와 과정을 간단하게 해주는 라이브러리입니다. PyTorch에서 자주 사용되던 부분들을 DataModule, Model 등으로 한 번에 구현할 수 있도록 되어 있습니다.

특히 학습에 필요한 엔지니어링을 담당하는 부분과 연구를 위한 모델 구현이 분리되어 있어서, 모델을 실험하고 평가할 때 신경 써야 하는 부분이 간소화되어 모델 개발에만 집중할 수 있었습니다. 따라서 실제 모델을 실험하는 과정에서는 forward 함수만 작성할 수 있도록 간략화하였습니다. 또한 실험에서 매번 변경되는 부분과 그렇지 않은 부분들을 분리하여, 실험에서 자주 변경되는 부분만 수정하고, 입출력 형태, 로깅, 메트릭 등은 자동으로 설정될 수 있도록 만들었습니다. 일관된 벤치마크도 만들어서 모델 성능 개선과 프로파일링에 도움이 되도록 했습니다.

추가로 [black](https://github.com/psf/black) 과 [isort](https://pycqa.github.io/isort/) 등을 [pre-commit](https://pre-commit.com/)으로 강제 하도록 하여 코드 가독성을 향상시켰습니다.
_더 빠른 [ruff](https://docs.astral.sh/ruff/)도 좋습니다._

![pre-commit workflow]({{ site.attach }}review-ncsoft/precommit_pipeline.png)

또한 테스트 코드를 적극적으로 도입했습니다. 테스트는 소프트웨어의 안전성과 신뢰성을 높이는 데 중요한 역할을 합니다. 테스트 코드는 코드의 기능을 검증하고, 코드의 변경 사항이 기존 기능에 영향을 미치지 않는지 확인하며, 테스트 코드를 작성하는 과정에서 코드의 설계를 개선할 수 있습니다.

특히 파이썬에서의 테스트 작성은 매우 중요합니다. 파이썬은 동적 타이핑 언어이기 때문에, 코드를 실행하기 전까지는 어떤 타입의 변수가 사용되는지 알 수 없습니다. 아무리 정적 분석 도구와 타입 힌트를 사용해도 실행 과정에서는 항상 사소한 문제가 발생할 수 있으며, 코드가 복잡해지고 모델이 거대해지면서 이를 하나하나 추적하는 것은 불가능에 가깝습니다. 이를 쉽게 확인하고 검증하기 위해 테스트 코드가 필요합니다.

하지만 머신러닝 모델은 데이터를 학습하고 예측하는 과정에서 많은 변수들이 존재하며, 이는 모델의 성능에 큰 영향을 미칠 수 있습니다. 모델은 데이터에 민감하고 대부분 확률적 결과를 얻기에 테스트에서 항상 원하는 결과를 얻지 못할 수 있습니다. 또한 모델의 학습과 추론은 단순한 코드에 비해 실행 시간이 길기 때문에 빠르게 테스트하기 힘듭니다. 따라서 연구 개발에 있어서 테스트 코드를 통해 확인해야 하는 부분이 무엇인지 파악하고 다음과 같은 목적을 달성하고자 했습니다.

1. 학습, 전처리, 추론등 모델의 기본동작이 잘 작동하는지 확인합니다.
2. 모델의 성능을 측정하고, 모델 구조 이외의 코드가 변경되어도 이전과 같은 결과를 얻을 수 있는지 확인합니다.
3. 서비스를 위해 패키징된 결과가 실험에서와 같은 결과를 얻을 수 있는지 확인합니다.
4. 학습, 추론, API등 시나리오에 대해서 프레임워크가 정상적으로 작동하는지 확인합니다.

더미 데이터를 통해 sanity check를 하여 개발 상황에서 빠르게 모델에 문제가 없음을 간단히 확인하도록 만들었고, CPU에서 적당한 수준에서 돌아갈 수 있는 규모의 데이터와 batch 크기로 모델을 학습시킨 결과를 통해 이후에 코드가 변경되었을 때 같은 결과를 얻을 수 있는지 확인하도록 CI를 구성했습니다.

이때 테스트 결과를 직접 모두 비교하면 precision, OS system, inference device, framework version 등의 다양한 요인으로 인해 미세한 오차가 발생해 테스트가 실패할 수 있기 때문에 특정 threshold를 초과하는지 검사하도록 했습니다. 모델이 추가될 때마다 기존 시나리오에서 잘 작동하는지 쉽게 테스트를 추가할 수 있도록 파이프라인을 구성하여 손쉽게 테스트 코드를 작성하고 실행할 수 있도록 만들어 모델의 안정성을 확인하고, 코드의 변경이 모델의 성능에 영향을 미치지 않는지 확인하도록 했습니다.

저는 연구자와 소프트웨어 엔지니어가 각각 더 잘하는 영역이 있다고 생각합니다. 다만 더 좋은 도구와 환경을 활용할 수 있음에도 그것을 잘 알지 못하거나, 익숙하지 않다는 이유로 먼 길을 돌아가는 경우도 자주 봐 왔기 때문에 더 쉽고 편하게 일을 할 수 있는 환경을 제공할 수 있다면 업무가 더 쉬워지고 실험 결과들이 견고해질 수 있다고 생각했습니다. 물론 저도 ML 연구자로서 업무를 진행하고 있지만, 평소에 관심이 있던 엔지니어링 역량을 활용해 개선을 혼자서라도 진행하면서 같이 일하는 연구자들이 더 쉽고 편하게 일을 할 수 있으면 좋겠다고 생각하여 이 작업을 진행할 수 있었습니다.
